---
title: "Lista 1 - Eduardo"
subtitle: "Cálculo de Probabilidade 2"
author: "Tailine J. S. Nonato"
format: pdf
---

# Exercício 1 
Um programa de TV dura 1 hora, e um telespectador impaciente vai mudar de canal a qualquer momento durante o programa (Isso significa que o instante em que ele mudará de canal é uma variável aleatória $X \sim U[0,1]$). Então considere as seguintes questões:

a. Qual a probabilidade dele assistir a maior parte do programa?

b. Se ele assistiu a maior parte do programa, qual a probabilidade dele desligar a TV ou mudar de canal nos últimos 10 minutos?

## Solução

a. A probabilidade dele assistir a maior parte do programa é dada por $P[X \leq 0.5] = 0.5$. Porque a variável aleatória $X$ é uniforme, então a probabilidade de ele assistir a maior parte do programa pode ser calculada como a área do retângulo formado pelo intervalo $[0,1]$ e a reta $y = 0.5$, logo $P[X \leq 0.5]$. Utilizando a função de distribuição acumulada da variável aleatória $X$, temos que $F_X(x) = x$, para $x \in [0,1]$. Portanto, $P[X \leq 0.5] = F_X(0.5) = 0.5$.

b. Dado que ele assistiu 50% ou mais do programa, a probabilidade dele ter assistido 50min ou mais do programa é $P[X \geq 0.9 | X \geq 0.5] = \frac{P[X \geq 0.9 \cap X \geq 0.5]}{P[X \geq 0.5]} = \frac{P[X \geq 0.9]}{P[X \geq 0.5]} = \frac{0.1}{0.5} = 0.2$. Portanto, a probabilidade dele desligar a TV ou mudar de canal nos últimos 10 minutos é $0.2$.

# Exercício 2
Considere as variáveis aleatórias X e Y onde X é discreta e Y é contínua com distribuição conjunta dada por

$$
f(x,y) = \begin{cases}
\frac{xy^{x-1}}{3}, & \text{se } x = 1,2,3 \text{ e } y \in [0,1] \\
0, & \text{caso contrário}
\end{cases}
$$

Calcule $F_Y (y|X = 2)$ e $FX (x|Y = 1/2)$.

## Solução

Para calcular $F_Y (y|X = 2)$, vamos utilizar a definição de densidade condicional:

> $f_Y (y|X = 2) = \frac{f(2,y)}{f_X(2)}$

Calculando $f_X(2)$

> $f_X(2) = \int_{0}^{1} f(2,y) dy$

> $f_X(2) = \int_{0}^{1} \frac{2y}{3} dy$

> $f_X(2) = \frac{1}{3}$

Calculando $f(2,y)$

> $f(2,y) = \frac{2y}{3}$

Portanto, 

> $F_Y (y|X = 2) = \frac{2y}{3} \cdot 3$ 

> $F_Y (y|X = 2)= 2y$

Para calcular $FX (x|Y = 1/2)$, vamos utilizar a definição de densidade condicional:

> $f_X (x|Y = 1/2) = \frac{f(x,1/2)}{f_Y(1/2)}$

Calculando $f_Y(1/2)$

> $f_Y(1/2) = \sum_{x=1}^{3} f(x,1/2)$

> $f_Y(1/2) = \frac{1}{6} + \frac{1}{4} + \frac{3}{8}$

> $f_Y(1/2) = \frac{11}{24}$

Calculando $f(x,1/2)$, com $x = 1,2,3$

> $f(1,1/2) = \frac{1}{6}$

> $f(2,1/2) = \frac{1}{4}$

> $f(3,1/2) = \frac{3}{8}$

Portanto,

> $f_X (x|Y = 1/2) = \frac{f(x,1/2)}{11/24}$

### Solução alternativa (em sala)

> $F_Y (y|X = 2) = \frac{P(Y \leq y, X = 2)}{P(X = 2)}$ 

Obtendo as distribuições marginais em $X$:

$P(X=1)$

> $P(X=1) = \int_{0}^{1} f(1,y) dy$

> $P(X=1) = \int_{0}^{1} \frac{1*y^{1-1}}{3} dy$

> $P(X=1) = \int_{0}^{1} \frac{1}{3} dy$

> $P(X=1) = \frac{1}{3}$

$P(X=2)$

> $P(X=2) = \int_{0}^{1} f(2,y) dy$

> $P(X=2) = \int_{0}^{1} \frac{2*y^{2-1}}{3} dy$

> $P(X=2) = \int_{0}^{1} \frac{2y}{3} dy$

> $P(X=2) = \frac{1}{3}$

$P(X=3)$

> $P(X=3) = \int_{0}^{1} f(3,y) dy$

> $P(X=3) = \int_{0}^{1} \frac{3*y^{3-1}}{3} dy$

> $P(X=3) = \int_{0}^{1} y^{2} dy$

> $P(X=3) = \frac{1}{3}$

Agora, calculando $P(X=2,Y \leq y)$

> $P(X=2,Y \leq y) = \int_{0}^{y} f(2,t) dt$

> $P(X=2,Y \leq y) = \int_{0}^{y} \frac{2t^{2-1}}{3} dt$

> $P(X=2,Y \leq y) = 2/3 \cdot \int_{0}^{y} t dt$

> $P(X=2,Y \leq y) = 2/3 \cdot \frac{t^2}{2} \Big|_{0}^{y}$

> $P(X=2,Y \leq y) = \frac{y^2}{3}$

Portanto,

$$
F_Y (y|X = 2) = \begin{cases}
0, & \text{se } y < 0 \\
y^2, & \text{se } 0 \leq y \leq 1 \\
1, & \text{se } y > 1
\end{cases}
$$


Se fosse solicitado para calcular $E(Y|X=2)$, teríamos que:

> $E(Y|X=2) = \int_{0}^{1} y \cdot dF(y|X=2)$

> $E(Y|X=2) = \int_{0}^{1} y \cdot 2y dy$

> $E(Y|X=2) = 2 \int_{0}^{1} y^2 dy$

> $E(Y|X=2) = 2 \cdot \frac{y^3}{3} \Big|_{0}^{1}$

> $E(Y|X=2) = \frac{2}{3}$

Agora calculando $F_X (x|Y = 1/2)$

> $F_X (x|Y = 1/2) = \frac{P(X \leq x, Y = 1/2)}{P(Y = 1/2)}$

Calcular $P(Y = 1/2)$ é incorreto porque $Y$ é contínua. Logo, a resposta correta é:

> $F_X (x|Y = 1/2) = \sum_{k=1}^{min(x,3)} f(k | y= 1/2)$

> $F_X (x|Y = 1/2) = \sum_{k=1}^{min(x,3)} \frac{f(x,1/2)}{f_Y(1/2)}$

Logo,

$$
F_X (x|Y = 1/2) = \begin{cases}
0, & x < 1 \\
4/11, & 1 \leq x < 2 \\
8/11, & 2 \leq x < 3 \\
1, & x \geq 3
\end{cases}
$$

Assim, pode-se verificar a função de densidade conjunta como:

$$
f(x,y) = \begin{cases}
\frac{xy^{x-1}}{3}, & \text{se } x = 1,2,3 \text{ e } y \in [0,1] \\
0, & \text{caso contrário}
\end{cases}
$$

E 

> $f(k,1/2) = \frac{k(1/2)}{3}$ para k = 1,2,3

> $f_Y(1/2) = \frac{(1/2)^0}{3} = \frac{1}{3}$

# Exercício 3
Sejam $X_1$ e $X_2$ variáveis aleatórias independentes, cada uma com distribuição geométrica dada por $P[X_1 = k] = P[X_2 = k] = p(1 −p)$ onde $k = 1,2,...,$ e $\theta < p < 1$. Então, calcule:

- a. $P[X_1 = X_2]$ e $P[X_1 < X_2]$;

- b. Calcule a distribuição condicional de $X_1$ dado $X_1 + X_2$;

## Solução

a. Para calcular $P[X_1 = X_2]$, vamos utilizar a definição de independência:

> $P[X_1 = X_2] = \sum_{k=1}^{\infty} P[X_1 = k] \cdot P[X_2 = k]$

> $P[X_1 = X_2] = \sum_{k=1}^{\infty} p(1-p) \cdot p(1-p)$

> $P[X_1 = X_2] = \sum_{k=1}^{\infty} p^2(1-p)^2$

> $P[X_1 = X_2] = p^2(1-p)^2 \sum_{k=1}^{\infty} 1$

> $P[X_1 = X_2] = p^2(1-p)^2$

Para calcular $P[X_1 < X_2]$, vamos utilizar a definição de independência:

> $P[X_1 < X_2] = \sum_{k=1}^{\infty} P[X_1 = k] \cdot P[X_2 > k]$

> $P[X_1 < X_2] = \sum_{k=1}^{\infty} p(1-p) \cdot (1 - p)^k$

> $P[X_1 < X_2] = p(1-p) \sum_{k=1}^{\infty} (1 - p)^k$

> $P[X_1 < X_2] = p(1-p) \cdot \frac{1-p}{1 - (1-p)}$

> $P[X_1 < X_2] = p(1-p) \cdot \frac{1-p}{p}$

> $P[X_1 < X_2] = (1-p)^2$

b. Para calcular a distribuição condicional de $X_1$ dado $X_1 + X_2$, vamos utilizar a definição de distribuição condicional:

> $P[X_1 = k|X_1 + X_2 = n] = \frac{P[X_1 = k, X_1 + X_2 = n]}{P[X_1 + X_2 = n]}$

> $P[X_1 = k|X_1 + X_2 = n] = \frac{P[X_1 = k, X_2 = n - k]}{P[X_1 + X_2 = n]}$

> $P[X_1 = k|X_1 + X_2 = n] = \frac{P[X_1 = k] \cdot P[X_2 = n - k]}{P[X_1 + X_2 = n]}$

> $P[X_1 = k|X_1 + X_2 = n] = \frac{p(1-p) \cdot p(1-p)}{P[X_1 + X_2 = n]}$

> $P[X_1 = k|X_1 + X_2 = n] = \frac{p^2(1-p)^2}{P[X_1 + X_2 = n]}$

Calculando $P[X_1 + X_2 = n]$

> $P[X_1 + X_2 = n] = \sum_{k=1}^{n-1} P[X_1 = k] \cdot P[X_2 = n - k]$

> $P[X_1 + X_2 = n] = \sum_{k=1}^{n-1} p(1-p) \cdot p(1-p)$

> $P[X_1 + X_2 = n] = p^2(1-p)^2 \sum_{k=1}^{n-1} 1$

> $P[X_1 + X_2 = n] = p^2(1-p)^2 \cdot (n-1)$

Portanto,

> $P[X_1 = k|X_1 + X_2 = n] = \frac{p^2(1-p)^2}{p^2(1-p)^2 \cdot (n-1)}$

> $P[X_1 = k|X_1 + X_2 = n] = \frac{1}{n-1}$


# Exercício 4
Uma certa lâmpada tem uma vida em horas, tendo distribuição exponencial de parâmetro $1$. Um jogador acende a lâmpada e, enquanto a lâmpada ainda estiver acesa, lança um dado equilibrado de 15 em 15 segundos. Qual o número esperado de 3's lançados pelo jogador até a lâmpada se apagar?

## Solução
Dividindo a resolução em passos, tem-se que:

1. Estabelecendo os objetos:

> $X$ = tempo de vida da lâmpada

> $X \sim Exp(1)$

> $N$ = número de lançamentos até a lâmpada se apagar

> $Y$ = número de "3"s lançados nos N lançamentos

2. Conectando informações:

> $N \sim Geom(1 - e^{-X})$

> $Y|N \sim Bin(N, 1/6)$

3. Calculando a esperança:

> $E(Y) = E(E(Y|N))$

> $E(Y|N) = N * 1/6$

> $E(Y) = E(N) * 1/6$

4. Calculando a esperança de $N$:

Considerando $\alpha=15$ segundos ou $\alpha=1/240$ horas, tem-se uma progressão geométrica com razão $e^{-\alpha}$, logo $P(X> n\alpha) = e^{-n\alpha}$. Então:

> $E(N) = \sum_{n=0}^{\infty} e^{-n\alpha}$

Por ser uma progressão geométrica, $\sum_{n=0}^{\infty} e^{-n\alpha}$ converge em $\frac{1}{1 - e^{-\alpha}}$, logo:

> $E(N) = \frac{1}{1 - e^{-\alpha}}$

5. Substituindo os valores:

> $E(Y) = \frac{1}{1 - e^{-\alpha}} * 1/6$

> $E(Y) = \frac{1}{1 - e^{-1/240}} * 1/6$

```{r}
ey = 1/(1 - exp(-1/240)) * 1/6
```

> $E(Y) \approx$ `r round(ey,3)`

# Exercício 5
Seja $X$ e $Y$ variáveis aleatórias independentes tais que $X \sim Binom(m,p)$ e $Y \sim Binom(n,p)$. 

Obtenha a distribuição condicional de $X$ dada $X + Y$. Como se chama essa distribuição?

## Solução

Para calcular a distribuição condicional de $X$ dado $X + Y$, vamos utilizar a definição de distribuição condicional:

> $P[X = k|X + Y = n] = \frac{P[X = k, X + Y = n]}{P[X + Y = n]}$

> $P[X = k|X + Y = n] = \frac{P[X = k, Y = n - k]}{P[X + Y = n]}$

> $P[X = k|X + Y = n] = \frac{P[X = k] \cdot P[Y = n - k]}{P[X + Y = n]}$

> $P[X = k|X + Y = n] = \frac{p(k) \cdot p(n - k)}{P[X + Y = n]}$


Calculando $P[X + Y = n]$

> $P[X + Y = n] = \sum_{k=0}^{n} P[X = k] \cdot P[Y = n - k]$

> $P[X + Y = n] = \sum_{k=0}^{n} p(k) \cdot p(n - k)$

> $P[X + Y = n] = \sum_{k=0}^{n} \binom{m}{k} p^k (1-p)^{m-k} \cdot \binom{n}{n-k} p^{n-k} (1-p)^{n-k}$

> $P[X + Y = n] = \sum_{k=0}^{n} \binom{m}{k} \binom{n}{n-k} p^n (1-p)^{m+n}$

> $P[X + Y = n] = p^n (1-p)^{m+n} \sum_{k=0}^{n} \binom{m}{k} \binom{n}{n-k}$

> $P[X + Y = n] = p^n (1-p)^{m+n} \sum_{k=0}^{n} \binom{m}{k} \binom{n}{k}$

> $P[X + Y = n] = p^n (1-p)^{m+n} \binom{m+n}{n}$

Portanto,

> $P[X = k|X + Y = n] = 

> $P[X = k|X + Y = n] = \frac{\binom{m}{k} \binom{n}{n-k}}{\binom{m+n}{n}}$

Essa distribuição é chamada de distribuição hipergeométrica.






# Exercício 6
Sejam $Y \sim Exp(1)$ e $(X|Y = y) \sim Poisson(y)$. Mostre que $P[X = n] = \frac{1}{2n+1}$, $n = 0,1,2,...$


## Solução

Para calcular $P[X = n]$, vamos utilizar a definição de distribuição condicional:

> $P[X = n] = \int_{0}^{\infty} P[X = n|Y = y] \cdot f_Y(y) dy$

> $P[X = n] = \int_{0}^{\infty} P[X = n|Y = y] \cdot e^{-y} dy$

> $P[X = n] = \int_{0}^{\infty} \frac{y^n e^{-y}}{n!} \cdot e^{-y} dy$

> $P[X = n] = \int_{0}^{\infty} \frac{y^n e^{-y}}{n!} \cdot e^{-y} dy$

> $P[X = n] = \frac{1}{n!} \int_{0}^{\infty} y^n e^{-y} dy$

> $P[X = n] = \frac{1}{n!} \int_{0}^{\infty} y^n e^{-y} dy$

> $P[X = n] = \frac{1}{n!} \Gamma(n+1)$

> $P[X = n] = \frac{1}{n!} \cdot n!$

> $P[X = n] = \frac{1}{n+1}$

Portanto, $P[X = n] = \frac{1}{2n+1}$, $n = 0,1,2,...$


# Exercício 7
Um contador recebe impulsos de duas fontes independentes, A e B. A fonte A gera impulsos segundo um processo de Poisson com parâmetro $\lambda > 0$, enquanto a fonte B gera impulsos segundo um processo de Poisson com parâmetro $\epsilon > 0$. Suponha que o contador registre todo o impulso gerado pelas duas fontes.

a. Seja $X_t$ o número de impulsos registrados até o tempo $t$, $t > 0$ ($X_0 = 0$). Explique porque $X_t$ tem distribuição Poisson. Qual parâmetro?

b. Qual a probabilidade de que o primeiro impulso gerado seja da fonte A?

c. Dado que exatamente 100 impulsos foram contados durante a primeira unidade de tempo, qual a distribuição que você atribui ao número emitido pela fonte A?

## Solução

a. Para explicar porque $X_t$ tem distribuição Poisson, vamos utilizar a definição de processo de Poisson:

> $P[X_t = n] = \frac{(\lambda t)^n e^{-\lambda t}}{n!}$

b. Para calcular a probabilidade de que o primeiro impulso gerado seja da fonte A, vamos utilizar a definição de probabilidade condicional:

> $P[A|A+B] = \frac{P[A \cap (A+B)]}{P[A+B]}$

> $P[A|A+B] = \frac{P[A]}{P[A+B]}$

> $P[A|A+B] = \frac{\lambda}{\lambda + \epsilon}$

c. Dado que exatamente 100 impulsos foram contados durante a primeira unidade de tempo, a distribuição que você atribui ao número emitido pela fonte A é uma distribuição binomial, com $n = 100$ e $p = \frac{\lambda}{\lambda + \epsilon}$. Portanto, $P[A = k] = \binom{100}{k} \left(\frac{\lambda}{\lambda + \epsilon}\right)^k \left(1 - \frac{\lambda}{\lambda + \epsilon}\right)^{100-k}$. 


# Exercício 8 (em sala)

Seja $X$ uma variável aleatória com função de densidade dada por:

$$
f(x) = \begin{cases}
\frac{1}{x^2}, & x > 1 \\
0, & \text{caso contrário}
\end{cases}
$$

E $B= \{X\leq 3\}$.

a. Calcule $F(X|B)$.

b. Calcule $f(x|Y=y)$

c. Calcule $E(X|Y=y)$

d. Calcule $F(X|Y=y)$

## Solução

a. Para calcular $F(X|B)$, vamos utilizar a definição de distribuição condicional:

> $F(X|B) = \frac{P(X \leq x, B)}{P(B)}$

> $F(X|B) = \frac{P(X \leq x, X \leq 3)}{P(X \leq 3)}$

i. Calculando $P(X \leq 3)$

> $P(X \leq 3) = \int_{-\infty}^{3} f(x) dx$

> $P(X \leq 3) = \int_{1}^{3} \frac{1}{x^2} dx$

> $P(X \leq 3) = -\frac{1}{x} \Big|_{1}^{3}$

> $P(X \leq 3) = -\frac{1}{3} + 1$

> $P(X \leq 3) = \frac{2}{3}$

ii. Calculando $P(X \leq x, X \leq 3)$

$$
P(X \leq x \cap X \leq 3) = \begin{cases}
P(X \leq x), & \text{se } x < 3 \\
P(X \leq 3), & \text{se } x \geq 3
\end{cases}
$$

Que é o mesmo que:

$$
P(X \leq x \cap X \leq 3) = \begin{cases}
\int_{-\infty}^{x} fa da, & \text{se } x < 3 \\
2/3, & \text{se } x \geq 3
\end{cases}
$$

Portanto,

$$
F(X|B) = \begin{cases}
0, & \text{se } x < 1 \\
1-\frac{1}{x}, & \text{se } 1 \leq x < 3 \\
2/3, & \text{se } x \geq 3
\end{cases}
$$
